---
title: "스타트캠프 - 데이터 크롤링(1)"
date: 2018-12-31 18:58
categories: [Start camp]
tags: ['Data_crawling', 'Python', 'Chatbot']
---


1. 크롤링이 무엇인가?
---

크롤링(crawling) 혹은 스크레이핑(scraping)은 웹 페이지를
그대로 가져와서 거기서 데이터를 추출해 내는 행위이다.
크롤링하는 소프트웨어는 크롤러 (crawler)라고 부른다.

**파이썬**이 이러한 작업에 적합하다고 한다.
파이썬이 언어 자체로도 굉장히 쉽기 때문에 비전공자들에게 진입장벽이 낮다고 하는데,
그래서인지 모르지만 스타트캠프에 파이썬을 선택한것으로 보인다.

우리는 Beautiful Soup ~~아름다운 국물~~ 라이브러리를 사용했다.



### HTML 태그와 Class를 이용해 원하는 부분 찾기

네이버 인기검색어의 TAG 와 Class 를 찾고 반복문을 통해 리스트에 저장하고, 리스트를 출력해봅니다.

반복문for 과 class_=“ ” 를 이용하여 네이버 인기 검색어를 저장항 리스트(list = [])에 담아 출력해봅니다.

<code>

from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()
def main():
    
    # URL 데이터를 가져올 사이트 url 입력
    url = "http://www.naver.com"
        
        
    #웹에 request 요청
    #url -> 필수적
    
    res = urllib.request.urlopen(url).read()
    
    # URL 주소에 있는 HTML 코드를 soup에 저장합니다.
    soup = BeautifulSoup(res, "html.parser")
    
    #list = c언어의 배열
    list = []  #빈 리스트를 만들고
    
    #["<span class='ah_k'>티르티르</span>"]
    for naver_text in soup.find_all('span', class_='ah_k')[:20]:  #20위까지만 볼거니까
        list.append(naver_text.get_text().strip())   #.strip()은 개행문자를 없애주는 메서드

    print(list)


if __name__ == "__main__":
    main()

</code>



### HTML 태그와 Class를 이용해 리스트 출력 연습

이전 실습과 마찬가지로 언론사 사이트의 기사 제목, 내용을 찾아서

웹 페이지에서 TAG와 Class를 찾아 리스트에 저장하고 출력하는 실습을 해봅니다.


<code>

from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    
    
    # URL 데이터를 가져올 사이트 url 입력
    url = "http://www.kyeonggi.com/?mod=news&act=articleList&view_type=S&sc_code=1439458030"

    # URL 주소에 있는 HTML 코드를 soup에 저장합니다.
    soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")


    ##### list_title과, list_content에 append()를 사용하여 원하는 정보를 하나씩 담아 출력합니다. #####

    list_title = []
    list_content = []

    for news_title in soup.find_all("div", class_="lost-titles"):
        list_title.append(new_title.get_text().strip())

    for news_content in soup.find_all("p", class_="list-summary") :
        list_content.append(news_content.get_text().strip())

    print(list_title)
    print(list_content)


    ##find는 제일 첫번째 하나만 가져오기
    ##find_all은 조건에 맞는 모든 검색결과를 가져오기


if __name__ == "__main__":
    main()

</code>



### HTML 태그와 Class를 이용해 리스트 출력 연습2


이전에 2번의 실습으로 익숙해진 HTML.

TAG 찾기와, Class 찾기로 커뮤니티 사이트의 웹 구조를 확인하여 제목과 글쓴이를 크롤링해서 가져오는 실습을 진행해봅니다.

    from elice_utils import EliceUtils
    import urllib.request
    from bs4 import BeautifulSoup

    elice_utils = EliceUtils()


    def main():
        print("커뮤니티 게시판")

        # URL 데이터를 가져올 사이트 url 입력
        url = "https://www.clien.net/service/group/community"

        # URL 주소에 있는 HTML 코드를 soup에 저장합니다.
        soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")

        ##### list_title와 list_nickname append()를 사용하여 원하는 정보를 하나씩 담아 출력합니다. #####

        list_title = []
        list_nickname = []

        for i in soup.find_all("span", class_="subject_fixed"):
            list_title.append(i.get_text().strip())  ##.strip()은 \n를 출력하지 않도록 해주는 

        for i in soup.find_all("span", class_="nickname"):
            if i.get_text().strip() == '':
                list_nickname.append(i.find("img")["alt"])
            else:
                list_nickname.append(i.get_text().strip())

        print(list_title)
        print(list_nickname)

    

    if __name__ == "__main__":
        main()



### 문제 1. 영화 후기 수집하기

영화 사이트에 있는 영화평을 수집해서 출력하는 문제를 실습해봅니다.

**나의 코드**
    from elice_utils import EliceUtils
    import urllib.request
    from bs4 import BeautifulSoup

    elice_utils = EliceUtils()


    def main():
        print("영화 리뷰")

        # 리뷰를 수집할 사이트 주소 입력
        url = "https://movie.naver.com/movie/bi/mi/review.nhn?code=168058#"


        # URL 주소에 있는 HTML 코드를 soup에 저장합니다.
        soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")

        # 1. 리뷰가 있는 있는 ul 태그 찾기
        # 2. ul 안에 있는 모든 li 태그 찾기
        # 3. li 태그 안에 있는 strong 태그에서 get_text()로 텍스트 추출  
    
        for i in soup.find_all("ul", class_="rvw_list_area"):
            print(i.get_text().strip())
  

    if __name__ == "__main__":
        main()


### 문제 2. 커뮤니티 댓글 수집하기

커뮤니티 댓글을 수집하여 출력하는 것을 실습해봅니다.

실습에서 쓰는 웹 사이트는 <https://pann.nate.com/talk/344083297>입니다.

	from elice_utils import EliceUtils
	import urllib.request
	from bs4 import BeautifulSoup

	elice_utils = EliceUtils()


	def main():
    		print("커뮤니티 댓글")

    		# 댓글를 수집할 사이트 주소 입력
    		url = "https://pann.nate.com/talk/344083297"

    		# URL 주소에 있는 HTML 코드를 soup에 저장합니다.
    		soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")

    		# 1. 댓글이 있는 태크 dd 찾기
    		# 2. class_="usertxt class 찾기"
    		# for 반복문과 get_text()를 사용해서 출력

    		for i in soup.find_all("dd", class_="usertxt"):
        			print(i.get_text().strip())


	if __name__ == "__main__":
    		main()



