---
title: "스타트캠프 - 데이터 크롤링(1)"
date: 2018-12-31 20:39
categories: [Start camp]
tags: ['Data_crawling', 'Python', 'Chatbot']
---


## 1. 크롤링이 무엇인가?

---

크롤링(crawling) 혹은 스크레이핑(scraping)은 웹 페이지를
그대로 가져와서 거기서 **데이터를 추출**해 내는 행위이다.
크롤링하는 소프트웨어는 **크롤러 (crawler)**라고 부른다.

**파이썬**이 이러한 작업에 적합하다고 한다.
파이썬이 언어 자체로도 굉장히 쉽기 때문에 비전공자들에게 진입장벽이 낮다고 하는데,
그래서인지 모르지만 스타트캠프에 파이썬을 선택한것으로 보인다.

우리는 Beautiful Soup ~~아름다운 국물~~ 라이브러리를 사용했다.



### HTML 태그와 Class를 이용해 원하는 부분 찾기

네이버 인기검색어의 TAG 와 Class 를 찾고 반복문을 통해 리스트에 저장하고, 리스트를 출력해봅니다.

반복문for 과 class_=“ ” 를 이용하여 네이버 인기 검색어를 저장항 리스트(list = [])에 담아 출력해봅니다.

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()

def main():
	# URL 데이터를 가져올 사이트 url 입력
    	url = "http://www.naver.com
        
	#웹에 request 요청
    	#url -> 필수적
    
    	res = urllib.request.urlopen(url).read()
    
    	# URL 주소에 있는 HTML 코드를 soup에 저장합니다.
    	soup = BeautifulSoup(res, "html.parser")
    		
    	#list = c언어의 배열
    	list = []  #빈 리스트를 만들고
    	
	#["<span class='ah_k'>티르티르</span>"]
    	for naver_text in soup.find_all('span', class_='ah_k')[:20]:  #20위까지만 볼거니까
        	list.append(naver_text.get_text().strip())   #.strip()은 개행문자를 없애주는 메서드
		
	print(list)
	

if __name__ == "__main__":
	main()
	
```



### HTML 태그와 Class를 이용해 리스트 출력 연습

이전 실습과 마찬가지로 언론사 사이트의 기사 제목, 내용을 찾아서

웹 페이지에서 TAG와 Class를 찾아 리스트에 저장하고 출력하는 실습을 해봅니다.

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
	# URL 데이터를 가져올 사이트 url 입력
	url = "http://www.kyeonggi.com/?mod=news&act=articleList&view_type=S&sc_code=1439458030"
	
	# URL 주소에 있는 HTML 코드를 soup에 저장합니다.
	soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")
		
		
    	##### list_title과, list_content에 append()를 사용하여 원하는 정보를 하나씩 담아 출력합니다. #####

    	list_title = []
    	list_content = []

	for news_title in soup.find_all("div", class_="lost-titles"):
    		list_title.append(new_title.get_text().strip())
	
    	for news_content in soup.find_all("p", class_="list-summary") :
      		list_content.append(news_content.get_text().strip())

	print(list_title)
    	print(list_content)


    	##find는 제일 첫번째 하나만 가져오기
    	##find_all은 조건에 맞는 모든 검색결과를 가져오기

if __name__ == "__main__":
	main()
```



### HTML 태그와 Class를 이용해 리스트 출력 연습2


이전에 2번의 실습으로 익숙해진 HTML.

TAG 찾기와, Class 찾기로 커뮤니티 사이트의 웹 구조를 확인하여 제목과 글쓴이를 크롤링해서 가져오는 실습을 진행해봅니다.

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()

def main():
    print("커뮤니티 게시판")

    # URL 데이터를 가져올 사이트 url 입력
    url = "https://www.clien.net/service/group/community"

    # URL 주소에 있는 HTML 코드를 soup에 저장합니다.
    soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")

    ##### list_title와 list_nickname append()를 사용하여 원하는 정보를 하나씩 담아 출력합니다. #####
    list_title = []
    list_nickname = []

    for i in soup.find_all("span", class_="subject_fixed"):
        list_title.append(i.get_text().strip())  ##.strip()은 \n를 출력하지 않도록 해주는 

    for i in soup.find_all("span", class_="nickname"):
        if i.get_text().strip() == '':
            list_nickname.append(i.find("img")["alt"])
        else:
            list_nickname.append(i.get_text().strip())

    print(list_title)
    print(list_nickname)

if __name__ == "__main__":
    main()
```


### 문제 1. 영화 후기 수집하기

영화 사이트에 있는 영화평을 수집해서 출력하는 문제를 실습해봅니다.

**내 코드**

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()

def main():
	print("영화 리뷰")

       	# 리뷰를 수집할 사이트 주소 입력
       	url = "https://movie.naver.com/movie/bi/mi/review.nhn?code=168058#"

       	# URL 주소에 있는 HTML 코드를 soup에 저장합니다.
       	soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")

       	# 1. 리뷰가 있는 있는 ul 태그 찾기
       	# 2. ul 안에 있는 모든 li 태그 찾기
       	# 3. li 태그 안에 있는 strong 태그에서 get_text()로 텍스트 추출  
   
       	for i in soup.find_all("ul", class_="rvw_list_area"):
       		print(i.get_text().strip())

if __name__ == "__main__":
       	main()
```

### 문제 2. 커뮤니티 댓글 수집하기

커뮤니티 댓글을 수집하여 출력하는 것을 실습해봅니다.

실습에서 쓰는 웹 사이트는 <https://pann.nate.com/talk/344083297>입니다.

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()

def main():
	print("커뮤니티 댓글")

	# 댓글를 수집할 사이트 주소 입력
	url = "https://pann.nate.com/talk/344083297"

	# URL 주소에 있는 HTML 코드를 soup에 저장합니다.
	soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")

	# 1. 댓글이 있는 태크 dd 찾기
    	# 2. class_="usertxt class 찾기"
    	# for 반복문과 get_text()를 사용해서 출력

	for i in soup.find_all("dd", class_="usertxt"):
        	print(i.get_text().strip())

if __name__ == "__main__":
	main()
```





## 2. 다양한 크롤링 기법

---

## Pagination
#### 제목에서 알 수 있듯이, page에 관련된 기법이다. 
Post list가 있을 때, page 수만큼 코드가 반복되는 것을 방지하기 위해 
loop를 이용해서 크롤링 하는 기법

### 1,2,3… 등으로 구분된 페이지 한번에 수집

실습을 진행하는 웹 사이트 주소입니다. 

<http://sports.donga.com/Enter?p=1&c=02>

여러 페이지로 구성된 웹 사이트의 게시글을 모두 한꺼번에 가져와서
필요한 정보만 리스트에 넣어서 출력해보는 실습을 진행해봅니다.

웹 url의 패턴을 찾아서 중첩 반복문을 이용해서 list에 1~5페이지까지의 뉴스 기사 제목을 한꺼번에 담아보는 실습을 진행해봅니다.

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
	list_pagination = []
	
	for i in range(0, 5):
		url = "http://sports.donga.com/Enter?p="+str((i*20)+1)+"&c=02"
	        req = urllib.request.Request(url)
	        sourcecode = urllib.request.urlopen(url).read()
	        soup = BeautifulSoup(sourcecode, "html.parser")

	        for i in soup.find_all("span", class_="tit"):
	            list_pagination.append(i.get_text().strip())

	print(list_pagination)


if __name__ == "__main__":
	main()
```



## Href 페이지 수집
  #### Post list에서 원하는 기사를 찾아 들어가 기사 내용을 크롤링하는 경우 사용하는 기법이다.


### Href 페이지 수집

이전 실습에서 진행된 1,2,3,4 — 으로 구성된 웹 사이트 형태뿐만이 아니라 
단일 페이지에 여러가지 링크가 있는 경우 링크로 이동해서 콘텐츠를 크롤링하기 위해
HTML Tag 중, 연동된 Href를 수집하여 출력하는 실습을 진행합니다.


```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    print("href 출력해보기")

    list_href = []

    url = "https://music.bugs.co.kr/chart"
    soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")

    # 반복문을 사용해 원하는 정보 range(3,23)까지 find("a")["href"] 를 사용해서
    # href 모두 수집하여 list_href에 저장

    for i in soup.find_all("span", class_="tit"):
        try:
            list_href.append(i.find("a")["href"].strip())
        except TypeError:
            pass

    print(list_href)


if __name__ == "__main__":
    main()
```


### 크롤링해서 수집해온 링크 주소에 있는 내용 크롤링

수집하는 페이지에 연동되어 있는 href를 추출하여
언론 기사 리스트 뿐 아니라, 전체 내용까지 수집하는법을 실습해봅니다.

첫번째로 href 링크를 모두 수집해서 len()함수를 이용하여 href를 수집한 리스트의 크기만큼 반복문을 실행해서
href로 연결된 페이지 모두 크롤링해오는 실습을 진행합니다.

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    list_href = []
    list_content = []

    url = "https://music.bugs.co.kr/chart"
    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    for href in soup.find_all("p", class_="title"):
        list_href.append("https://news.sbs.co.kr"+ str(href.find("a")["href"]))

    for i in range(0, len(list_href)):
        url = list_href[i]
        #print(url)
        req = urllib.request.Request(url)
        sourcecode = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(sourcecode, "html.parser")

        list_content.append(soup.find("div", class_="text_area").get_text().strip())

    print(keywords)


if __name__ == "__main__":
    main()
```


### 언론사 기사 내용 포함여부 확인하기

이전 실습해서 수집해온 언론사 기사 내용 중, 특정 텍스트를 포함하고 있는지
검사하는 프로그램을 제작하여 포함하고 있다면 Okay, 포함하지 않고 있다면 No를 출력하는 프로그램을 제작합니다.

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    list = []
    list_content = []

    url = "https://news.sbs.co.kr/news/newsflash.do?plink=GNB&cooper=SBSNEWS"
    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    for i in soup.find_all("div", class_="mfn_inner"):
        list.append("https://news.sbs.co.kr"+str(i.find("a")["href"]))

    for i in range(0, len(list)):
        url = list[i]
        req = urllib.request.Request(url)
        sourcecode = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(sourcecode, "html.parser")

        list_content.append(soup.find("div", class_="text_area").get_text())

    for i in list_content:

        if " 가 " in i:
            print("Okay")
        else:
            print("No")


if __name__ == "__main__":
    main()

```

### 문제 1 href 수집하기

웹 페이지 href 링크들을 수집하여 출력해봅니다.

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    print("최신 뉴스 기사 href 수집")

    # href 수집할 사이트 주소 입력
    url = "https://news.nate.com/recent?mid=n0100"

    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    # 1. a 태그가 있는 div 태그 및 class 찾기
    # 2. find("a")["href"]로 href 추출
    # 3. 문자열 http: 를 앞에 추가해줌으로써 완벽한 링크 만들기

    for i in soup.find_all("div", class_="mlt01"):
        print("https:"+str(i.find("a")["href"]))


if __name__ == "__main__":
    main()
```


### 문제 2 href 수집 후, 링크 안에 내용 수집하기

이전 문제에서 수집한 href 링크에 있는 내용만을 추출해봅니다.

```python
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    print("최신 뉴스 기사 href 안의 내용 수집")

    # href 수집할 사이트 주소 입력
    url = "https://news.nate.com/recent?mid=n0100"

    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    # 1. a 태그가 있는 div 태그 및 class 찾기
    # 2. find("a")["href"]로 href 추출
    # 3. 문자열 http: 를 앞에 추가해줌으로써 완벽한 링크 만들기
    # 4. list에 넣기

    links = ["https:" + i.find("a")["href"] for i in soup.find_all("div", class_="mlt01")]
    
    # 1. list의 크기 만큼 for 루프 출력
    # 2. url에 list에 있는 href 링크 하나씩 넣어서 출력하기

    for i in range(len(links)):
        url = links[i]
        req = urllib.request.Request(url)
        sourcecode = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(sourcecode, "html.parser")
        try:
            print(soup.find("h3", class_="articleSubecjt").get_text().strip())
            
        except AttributeError:
            pass


if __name__ == "__main__":
    main()
```





## 3. 텍스트 가공 기법

정규 표현식과 같은 표현법을 이용해 조건에 맞는 문자열을 찾아 크롤링하는 기법

### 간단한 문자열 테크닉 실습
문자열을 가공하는 기법에 대해 간단하게 몇가지 예제를 실습해봅니다.

“문자열” + “문자열” 로 문자열을 합쳐보거나 split()함수를 이용해서 문장을 단어별로 분리해보거나

replace() 함수를 이용해서, 문자열을 대체해보는 실습을 해봅니다.

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()

def main():
    text_one = "나는 짜장면을 좋아합니다."
    splited = text_one.split()
    print(splited)
    
    print(text_one.replace("짜장면", "짬뽕"))

if __name__ == "__main__":
    main()
```

### 크롤링해온 텍스트의 문자열 가공
데이터를 크롤링해서 수집하고, 문자열을 가공하여 원하는 데이터만 남겨서 출력하는 실습을 진행해봅니다.

언론사 사이트에서 제목을 크롤링해와서 “…”로 잘린 부분까지의 문장을

“…”까지의 문자 위치를 찾아, “…”뒤의 문장을 삭제하고 “…”앞 까지의 문장만 저장해봅니다.

“…”이 없는 부분까지 if-else 를 통하여 예외처리하는 법을 실습해봅니다.

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()
import urllib.request
from bs4 import BeautifulSoup


def main():
    url = "http://www.newsis.com/eco/list/?cid=10400&scid=10404"
    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    for text in soup.find_all("strong", class_="title"):

        num = text.get_text().find("…") #...이 없으면 -1이 return

        if num != -1:
            print(text.get_text()[:num])
        else:
            print(text.get_text())


if __name__ == "__main__":
    main()
```

### 정규식 알아보기
정규식을 사용하여, 원하는 정보만 수집하는 방법과 수집한 데이터가 원하는 형태인지 검사하는 방법을 알아봅니다.

“010-000-0000” 와 같은 숫자 3개 3개 4개 로 구성된 문자열만을 문장에서 찾아 출력해보거나

elice@elice.com과 같은 이메일의 양식이 올바른지 검사하는 프로그램을 만들어봅니다.

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()
import re     #regular expression


def main():
    sentence = "제보는 032-232-3245 또는 010-222-2233 으로 연락주시기 바랍니다."

    compile_text = re.compile("\d\d\d-\d\d\d-\d\d\d\d")
    match_text = compile_text.findall(sentence)  #findall -> 정규표현식과 매칭되는 모든 부분을 리스트로 반환
    print(match_text)

    email = ["aa2@naver.com", "kbc@aaaaa", "Alice@Alice.com", "ILove@CodingLove"]

    compile_text = re.compile('.+@.+')

    for i in email:
        print(compile_text.findall(i))


if __name__ == "__main__":
    main()
```

### 엑셀 시트로 저장하기
실생활에서는 정보를 엑셀에서 다루는 경우가 많습니다.

웹 상에서 크롤링해 온 정보를 openpyxl 라이브러리를 사용해서

엑셀 row, column 위치를 지정해주고 알맞게 저장하는 방법을 실습해봅니다.

```python
import urllib.request

from bs4 import BeautifulSoup
from openpyxl import Workbook
from elice_utils import EliceUtils

elice_utils = EliceUtils()


def main():
    wb = Workbook()

    sheet1 = wb.active
    file_name = 'sample.xlsx'

    url = "http://www.newsis.com/eco/list/?cid=10400&scid=10404"
    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    articles = []

    for i in soup.find_all("strong", class_="title"):
        articles.append(i.get_text())

    for row_index in range(1, len(articles)+1):
        sheet1.cell(row=row_index, column=1).value = row_index
        sheet1.cell(row=row_index, column=2).value = articles[row_index-1]

    wb.save(filename=file_name)
    elice_utils.send_file(file_name)


if __name__ == "__main__":
    main()
```

### 문제 1 문자열 가공 실습

간단하게 몇가지 문자열 실습을 해봅니다.

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()


def main():
    sentence1 = "오늘은 날씨가 좋네요."
    sentence2 = "서울에 날씨가 좋네요."
    sentence3 = "부산에 날씨가 나쁘네요."

    # replace를 이용해서 "날씨"를 "공기"로 변경해서 출력해보기

    print(sentence1.replace("날씨", "공기"))
    print(sentence2.replace("날씨", "공기"))
    print(sentence3.replace("날씨", "공기"))

    # split을 이용하여 문장을 끊어서
    # "오늘은 서울에 날씨가 나쁘네요." 문장 만들어보기

    print(sentence1.split()[0] + " "+ sentence2.split()[0] +" "+ sentence3.split()[1] + " " + sentence3.split()[2])


if __name__ == "__main__":
    main()
```

### 문제 2 정규식 문제
우리는 앞 수업에서 정규식을 배워보았습니다.

이제 직접 정규식을 통해 원하는 정보만을 가져와보는 문제를 풀어봅니다.

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()
import re


def main():
    # 정규식을 통해, 사람들의 연락처 중 010을 제거하고 남은 부분
    # ex 01012341234 중 12341234를 추출하는 문제를 풀어봅니다

    # 정규식을 이용해 숫자만을 추출한 후, 010 부분을 제거합니다.

    sentence = "앨리스 : 01012341234,홍길동 01011112222, 박보검 01088885555"

    compile_text = re.compile("\d+")
    match_text = compile_text.findall(sentence)

    numbers = []

    numbers.append(match_text[0][3:])
    numbers.append(match_text[1][3:])
    numbers.append(match_text[2][3:])

    print(numbers)


# 결과물 : 12341234, 11112222, 88885555


if __name__ == "__main__":
    main()
```





## 4. 데이터 시각화

### 간단한 차트 한번 그려보기

matplotlib 라이브러리를 사용해서 간단하게 차트를 그리는 코딩을 해봅니다.
파이썬에서 데이터를 가지고 시각화가 가능하다는 것을 직접 코딩으로 알아봅니다.


출력예시

![](https://kasausyrzlhe1066469.cdn.ntruss.com/file/t/70cb0591f47e4cf0856141833a506350/a.png)

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()
import matplotlib.pyplot as plt
import numpy as np


def main():
    x = [1,2,3,4,5,6,7,8,9]
    y = [1,2,3,4,5,6,7,8,9]

    plt.plot(x, y)
    plt.xlabel('x_chuck')
    plt.ylabel('y_chuck')
    plt.title('sample_chart')

    plt.savefig("image.svg", formate="svg")
    elice_utils.send_image("image.svg")


if __name__ == "__main__":
    main()
```



### 비교 차트 그리기
2가지의 데이터셋을 시각화해서 비교하는 방법을 실습해봅니다.

Java와 Python을 a,b,c,d,e라는 5명의 학생의 점수를

한눈에 알아보기 쉽게 그래프를 그려봅니다.

출력예시

![](https://kasausyrzlhe1066469.cdn.ntruss.com/file/t/8cefc1f9358843448d9933954c51ce50/a.png)


```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()
from matplotlib import pyplot as plt


def main():
    print("비교 차트 그리기")

    plt.xlabel("person")
    plt.ylabel("grade")

    # Python
    plt.plot(['a', 'b', 'c', 'd', 'e'],[100,97,46,28,84], 'v')

    # Java
    plt.plot(['a', 'b', 'c', 'd', 'e'],[58,28,37,83,22])

    plt.savefig("image.svg", format="svg")
    elice_utils.send_image("image.svg")


if __name__ == "__main__":
    main()
```


### 다양한 차트 그려보기
선으로 된 차트 뿐만이 아니라 산포도, 파이차트를 직접 그려보면서

다양한 차트를 그리는 것이 가능하다는 것을 알아보고 차트 그리는 것에 익숙해집니다.

출력예시

![](https://kasausyrzlhe1066469.cdn.ntruss.com/file/t/2568303dfdcd45ecbe6a5a6fda508a8a/b.png)
![](https://kasausyrzlhe1066469.cdn.ntruss.com/file/t/ed6a9c969b6c48248179ad595ff2e937/a.png)

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()
import numpy as np
import matplotlib.pyplot as plt


def main():
    print("----산포도1----")

    x = np.arange(0, 100)
    y = np.arange(0, 100)
    #plt.plot(x, y, "o")     #plt.plot(x, y, "k") 
    plt.savefig("image.svg", format="svg")
    elice_utils.send_image("image.svg")

    print("----산포도2----")

    line = plt.figure()

    np.random.seed(5)
    x = np.arange(1, 101)
    y = 20 + 3 * x + np.random.normal(0, 40, 100)
    plt.plot(x, y, "o")
    plt.savefig("image.svg", format="svg")
    elice_utils.send_image("image.svg")
    #plt.show()

    print("----파이차트----")

    slices_hours = [12.5, 25.0, 62.5]
    activities = ["Sleep", "Work", "Nothing"]

    color = ["gray", "blue", "red"]

    plt.pie(slices_hours, labels=activities, colors=color, startangle=90, autopct='%.1f%%')

    plt.savefig("image.svg", format="svg")
    elice_utils.send_image("image.svg")


if __name__ == "__main__":
    main()
```


### 공공데이터를 시각화해보기

엑셀파일을 가져와 반복문을 통해서 엑셀파일에 있는 내용을 리스트에 저장하고나서

리스트에 저장된 데이터를 시각화해보는 실습을 진행해봅니다.

출력예시

![](https://kasausyrzlhe1066469.cdn.ntruss.com/file/t/af443bc11a834193864935f952772a58/a.png)

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()

import openpyxl
from matplotlib import pyplot as plt


def main():
    # 엑셀파일 열기
    wb = openpyxl.load_workbook('data.xlsm')

    # 현재 Active Sheet 얻기
    ws = wb.active

    year = []
    grade1 = []
    grade2 = []
    grade3 = []

    # 점수 읽기 row
    for r in ws.columns:
        year.append(r[0].value)
        grade1.append(r[1].value)
        grade2.append(r[2].value)
        grade3.append(r[3].value)

    plt.xlabel('year')
    plt.ylabel('grade')

    plt.plot(year, grade1)
    plt.plot(year, grade2)
    plt.plot(year, grade3)

    plt.savefig("image.svg", format="svg")
    elice_utils.send_image("image.svg")


if __name__ == "__main__":
    main()
```

### 뉴스시간 시각화

마지막 실습으로 현재까지 배운 것을 크롤링, 데이터 가공, 시각화 모두 이용하여 프로그램을 제작해봅니다.

뉴스가 올라오는 시간대를 크롤링하고 원하는 부분만 남기고 문자열을 가공하여, 파이 차트로 표현해봅니다.

출력예시

![](https://kasausyrzlhe1066469.cdn.ntruss.com/file/t/12b9f027ffdb40e1a25d5b4f1ac06f7e/c.png)

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()

import urllib.request
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt

def main():
	
    url = "http://news.jtbc.joins.com/section/index.aspx?scode=20"
    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")


    time_list = []


    for i in range(3,22):
        time_list.append(soup.find_all("span", class_="date")[i].get_text().strip())

    time_edit = []

    for i in range(0,len(time_list)) :
        time_edit.append(time_list[i][8:10])


    first_count = 0
    second_count = 0

    for i in range(0,len(time_edit)) :

        if (time_edit[i] == "03") :
            first_count = first_count + 1
        else :
            second_count = second_count + 1

    print(first_count)
    print(second_count)

    slices_hours = [first_count, second_count]
    activities = ['first_day', 'second_day']
    colors = [ 'gray', 'blue']
    plt.pie(slices_hours, labels=activities, colors=colors, startangle=90, autopct='%.1f%%')
    
    plt.savefig("image.svg", format="svg")
    elice_utils.send_image("image.svg")


if __name__ == "__main__":
    main()

```