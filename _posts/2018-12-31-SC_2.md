---
title: "스타트캠프 - 데이터 크롤링(2)"
date: 2018-12-31 20:42
categories: [Start camp]
tags: ['Data_crawling', 'Python', 'Chatbot']
---


2. 다양한 크롤링 기법

---

+ Pagination

    +제목에서 알 수 있듯이, page에 관련된 기법이다. 
Post list가 있을 때, page 수만큼 코드가 반복되는 것을 방지하기 위해 
loop를 이용해서 크롤링 하는 기법

### 1,2,3… 등으로 구분된 페이지 한번에 수집

실습을 진행하는 웹 사이트 주소입니다. 

<http://sports.donga.com/Enter?p=1&c=02>

여러 페이지로 구성된 웹 사이트의 게시글을 모두 한꺼번에 가져와서
필요한 정보만 리스트에 넣어서 출력해보는 실습을 진행해봅니다.

웹 url의 패턴을 찾아서 중첩 반복문을 이용해서 list에 1~5페이지까지의 뉴스 기사 제목을 한꺼번에 담아보는 실습을 진행해봅니다.

	from elice_utils import EliceUtils
	import urllib.request
	from bs4 import BeautifulSoup

	elice_utils = EliceUtils()


	def main():
	    list_pagination = []

	    for i in range(0, 5):
	        url = "http://sports.donga.com/Enter?p="+str((i*20)+1)+"&c=02"
	        req = urllib.request.Request(url)
	        sourcecode = urllib.request.urlopen(url).read()
	        soup = BeautifulSoup(sourcecode, "html.parser")

	        for i in soup.find_all("span", class_="tit"):
	            list_pagination.append(i.get_text().strip())

	    print(list_pagination)


	if __name__ == "__main__":
	    main()


+ Href 페이지 수집
    + Post list에서 원하는 기사를 찾아 들어가 기사 내용을 크롤링하는 경우 사용하는 기법이다.


### Href 페이지 수집

이전 실습에서 진행된 1,2,3,4 — 으로 구성된 웹 사이트 형태뿐만이 아니라 
단일 페이지에 여러가지 링크가 있는 경우 링크로 이동해서 콘텐츠를 크롤링하기 위해
HTML Tag 중, 연동된 Href를 수집하여 출력하는 실습을 진행합니다.


<code>
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    print("href 출력해보기")

    list_href = []

    url = "https://music.bugs.co.kr/chart"
    soup = BeautifulSoup(urllib.request.urlopen(url).read(), "html.parser")

    # 반복문을 사용해 원하는 정보 range(3,23)까지 find("a")["href"] 를 사용해서
    # href 모두 수집하여 list_href에 저장

    for i in soup.find_all("span", class_="tit"):
        try:
            list_href.append(i.find("a")["href"].strip())
        except TypeError:
            pass

    print(list_href)


if __name__ == "__main__":
    main()
</code>


### 크롤링해서 수집해온 링크 주소에 있는 내용 크롤링

수집하는 페이지에 연동되어 있는 href를 추출하여
언론 기사 리스트 뿐 아니라, 전체 내용까지 수집하는법을 실습해봅니다.

첫번째로 href 링크를 모두 수집해서 len()함수를 이용하여 href를 수집한 리스트의 크기만큼 반복문을 실행해서
href로 연결된 페이지 모두 크롤링해오는 실습을 진행합니다.

<code>
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    list_href = []
    list_content = []

    url = "https://music.bugs.co.kr/chart"
    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    for href in soup.find_all("p", class_="title"):
        list_href.append("https://news.sbs.co.kr"+ str(href.find("a")["href"]))

    for i in range(0, len(list_href)):
        url = list_href[i]
        #print(url)
        req = urllib.request.Request(url)
        sourcecode = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(sourcecode, "html.parser")

        list_content.append(soup.find("div", class_="text_area").get_text().strip())

    print(keywords)


if __name__ == "__main__":
    main()
</code>


### 언론사 기사 내용 포함여부 확인하기

이전 실습해서 수집해온 언론사 기사 내용 중, 특정 텍스트를 포함하고 있는지
검사하는 프로그램을 제작하여 포함하고 있다면 Okay, 포함하지 않고 있다면 No를 출력하는 프로그램을 제작합니다.

<code>
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    list = []
    list_content = []

    url = "https://news.sbs.co.kr/news/newsflash.do?plink=GNB&cooper=SBSNEWS"
    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    for i in soup.find_all("div", class_="mfn_inner"):
        list.append("https://news.sbs.co.kr"+str(i.find("a")["href"]))

    for i in range(0, len(list)):
        url = list[i]
        req = urllib.request.Request(url)
        sourcecode = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(sourcecode, "html.parser")

        list_content.append(soup.find("div", class_="text_area").get_text())

    for i in list_content:

        if " 가 " in i:
            print("Okay")
        else:
            print("No")


if __name__ == "__main__":
    main()

</code>

### 문제 1 href 수집하기

웹 페이지 href 링크들을 수집하여 출력해봅니다.

<code>
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    print("최신 뉴스 기사 href 수집")

    # href 수집할 사이트 주소 입력
    url = "https://news.nate.com/recent?mid=n0100"

    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    # 1. a 태그가 있는 div 태그 및 class 찾기
    # 2. find("a")["href"]로 href 추출
    # 3. 문자열 http: 를 앞에 추가해줌으로써 완벽한 링크 만들기

    for i in soup.find_all("div", class_="mlt01"):
        print("https:"+str(i.find("a")["href"]))


if __name__ == "__main__":
    main()
</code>


### 문제 2 href 수집 후, 링크 안에 내용 수집하기

이전 문제에서 수집한 href 링크에 있는 내용만을 추출해봅니다.

<code>
from elice_utils import EliceUtils
import urllib.request
from bs4 import BeautifulSoup

elice_utils = EliceUtils()


def main():
    print("최신 뉴스 기사 href 안의 내용 수집")

    # href 수집할 사이트 주소 입력
    url = "https://news.nate.com/recent?mid=n0100"

    req = urllib.request.Request(url)
    sourcecode = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(sourcecode, "html.parser")

    # 1. a 태그가 있는 div 태그 및 class 찾기
    # 2. find("a")["href"]로 href 추출
    # 3. 문자열 http: 를 앞에 추가해줌으로써 완벽한 링크 만들기
    # 4. list에 넣기

    links = ["https:" + i.find("a")["href"] for i in soup.find_all("div", class_="mlt01")]
    
    # 1. list의 크기 만큼 for 루프 출력
    # 2. url에 list에 있는 href 링크 하나씩 넣어서 출력하기

    for i in range(len(links)):
        url = links[i]
        req = urllib.request.Request(url)
        sourcecode = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(sourcecode, "html.parser")
        try:
            print(soup.find("h3", class_="articleSubecjt").get_text().strip())
            
        except AttributeError:
            pass


if __name__ == "__main__":
    main()
</code>